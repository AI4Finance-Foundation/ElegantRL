{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_helloworld_DQN_DDPG_PPO.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/ElegantRL/blob/master/tutorial_helloworld_DQN_DDPG_PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1gUG3OCJ5GS"
      },
      "source": [
        "# Demo: ElegantRL_HelloWorld_tutorial (DQN --> DDPG --> PPO)\n",
        "\n",
        "We suggest to following this order to quickly learn about RL:\n",
        "- DQN (Deep Q Network), a basic RL algorithms in discrete action space.\n",
        "- DDPG (Deep Deterministic Policy Gradient), a basic RL algorithm in continuous action space.\n",
        "- PPO (Proximal Policy Gradient), a widely used RL algorithms in continuous action space.\n",
        "\n",
        "If you have any suggestion about ElegantRL Helloworld, you can discuss them in [ElegantRL issues/135: Suggestions for elegant_helloworld](https://github.com/AI4Finance-Foundation/ElegantRL/issues/135), and we will keep an eye on this issue.\n",
        "ElegantRL's code, especially the Helloworld, really needs a lot of feedback to be better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbamGVHC3AeW"
      },
      "source": [
        "# **Part 1: Install ElegantRL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U35bhkUqOqbS"
      },
      "source": [
        "# install elegantrl library\n",
        "!pip install git+https://github.com/AI4Finance-LLC/ElegantRL.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Part 2: Import ElegantRL helloworld**\n",
        "\n",
        "We hope that the `ElegantRL Helloworld` would help people who want to learn about reinforcement learning to quickly run a few introductory examples.\n",
        "- **Less lines of code**. (code lines <1000)\n",
        "- **Less packages requirements**. (only `torch` and `gym` )\n",
        "- **keep a consistent style with the full version of ElegantRL**.\n",
        "\n",
        "![File_structure of ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL/raw/master/figs/File_structure.png)\n",
        "\n",
        "One sentence summary: an agent `agent.py` with Actor-Critic networks `net.py` is trained `run.py` by interacting with an environment `env.py`.\n",
        "\n",
        "\n",
        "In this tutorial, we only need to download the directory from [helloworld](https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/helloworld) using the following code.\n",
        "\n",
        "The files in `elegantrl_helloworld` including:\n",
        "`config.py`, `agent.py`, `net.py`, `env.py`, `run.py`"
      ],
      "metadata": {
        "id": "zJPivVxHMrAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r -f /content/ElegantRL/\n",
        "!rm -r -f /content/elegantrl_helloworld\n",
        "!git clone https://github.com/AI4Finance-Foundation/ElegantRL.git\n",
        "!mv /content/ElegantRL/helloworld /content/elegantrl_helloworld"
      ],
      "metadata": {
        "id": "sw_gE-IpovQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.erl_run import train_agent, valid_agent\n",
        "from elegantrl_helloworld.erl_config import Config, get_gym_env_args"
      ],
      "metadata": {
        "id": "nweGpiR1M0yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVdmpnK_3Zcn"
      },
      "source": [
        "## **Part 3: Train DQN on discreted action space task.**\n",
        "\n",
        "Train DQN on [**Discreted action** space task `CartPole`](https://gym.openai.com/envs/CartPole-v1/)\n",
        "\n",
        "You can see [/helloworld/erl_config.py](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/helloworld/erl_config.py) to get more information about hyperparameter.\n",
        "\n",
        "```\n",
        "class Arguments:\n",
        "    def __init__(self, agent_class, env_func=None, env_args=None):\n",
        "        self.env_num = self.env_args['env_num']  # env_num = 1. In vector env, env_num > 1.\n",
        "        self.max_step = self.env_args['max_step']  # the max step of an episode\n",
        "        self.env_name = self.env_args['env_name']  # the env name. Be used to set 'cwd'.\n",
        "        self.state_dim = self.env_args['state_dim']  # vector dimension (feature number) of state\n",
        "        self.action_dim = self.env_args['action_dim']  # vector dimension (feature number) of action\n",
        "        self.if_discrete = self.env_args['if_discrete']  # discrete or continuous action space\n",
        "        ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.erl_config import Config\n",
        "from elegantrl_helloworld.erl_agent import AgentDQN\n",
        "agent_class = AgentDQN\n",
        "env_name = \"CartPole-v1\"\n",
        "\n",
        "import gymnasium as gym\n",
        "# gym.logger.min_level(40)  # Block warning\n",
        "env = gym.make(env_name)\n",
        "env_func = gym.make\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Config(agent_class, env_func, env_args)\n",
        "\n",
        "# Set attributes from env_args on the args object\n",
        "args.state_dim = env_args['state_dim']\n",
        "args.action_dim = env_args['action_dim']\n",
        "args.if_discrete = env_args['if_discrete']\n",
        "# args.env_num = env_args['env_num'] # Assuming env_num is in env_args based on LunarLander example output\n",
        "\n",
        "\n",
        "'''reward shaping'''\n",
        "args.reward_scale = 2 ** 0  # an approximate target reward usually be closed to 256\n",
        "args.gamma = 0.97  # discount factor of future rewards\n",
        "\n",
        "'''network update'''\n",
        "args.net_dim = 2 ** 7  # the middle layer dimension of Fully Connected Network\n",
        "args.num_layer = 3  # the layer number of MultiLayer Perceptron, `assert num_layer >= 2`\n",
        "args.batch_size = 2 ** 7  # num of transitions sampled from replay buffer.\n",
        "args.repeat_times = 2 ** 0  # repeatedly update network using ReplayBuffer to keep critic's loss small\n",
        "args.explore_rate = 0.25  # epsilon-greedy for exploration.\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 5  # number of times that get episode return\n",
        "args.eval_times = 2 ** 3  # number of times that get episode return\n",
        "args.break_step = int(8e4)  # break training if 'total_step > break_step'"
      ],
      "metadata": {
        "id": "AAPdjovQrTpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose gpu id `0` using `args.learner_gpu = 0`. Set as `-1` or GPU is unavaliable, the training program will choose CPU automatically.\n",
        "\n",
        "- The cumulative returns of CartPole-v0  is ∈ (0, (1, 195), 200)\n",
        "- The cumulative returns of task_name is ∈ (min score, (score of random action, target score), max score)."
      ],
      "metadata": {
        "id": "Rq5LPOH2B0aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.erl_run import train_agent, valid_agent\n",
        "from elegantrl_helloworld.erl_config import get_gym_env_args\n",
        "from elegantrl_helloworld.erl_config import Config\n",
        "\n",
        "args.learner_gpus = -1\n",
        "\n",
        "train_agent(args)\n",
        "valid_agent(args) # Changed evaluate_agent to valid_agent\n",
        "print('| The cumulative returns of CartPole-v0  is ∈ (0, (1, 195), 200)')"
      ],
      "metadata": {
        "id": "n7SBwVAkA8lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train DQN on [**Discreted action** space env `LunarLander`](https://gym.openai.com/envs/LunarLander-v2/)\n",
        "\n",
        "**You can pass and run codes below.**. Because DQN takes over 6000 seconds for training. It is too slow. (DuelingDoubleDQN taks less than 1000 second for training on LunarLander-v2 task.)\n",
        "\n",
        "And there are many other DQN variance algorithms which get higher cumulative returns and takes less time for training. See [examples/demo_DQN_Dueling_Double_DQN.py](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/examples/demo_DQN_Dueling_Double_DQN.py)"
      ],
      "metadata": {
        "id": "qK21xTxnHGOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.erl_agent import AgentDQN\n",
        "agent_class = AgentDQN\n",
        "env_name = \"LunarLander-v3\"\n",
        "\n",
        "import gym\n",
        "gym.logger.set_level(40)  # Block warning\n",
        "env = gym.make(env_name)\n",
        "env_func = gym.make\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Arguments(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.reward_scale = 2 ** 0\n",
        "args.gamma = 0.99\n",
        "\n",
        "'''network update'''\n",
        "args.target_step = args.max_step\n",
        "args.net_dim = 2 ** 7\n",
        "args.num_layer = 3\n",
        "\n",
        "args.batch_size = 2 ** 6\n",
        "\n",
        "args.repeat_times = 2 ** 0\n",
        "args.explore_noise = 0.125\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 7\n",
        "args.eval_times = 2 ** 4\n",
        "args.break_step = int(4e5)  # LunarLander needs a larger `break_step`\n",
        "\n",
        "args.learner_gpus = -1  # denotes use CPU\n",
        "train_agent(args)\n",
        "valid_agent(args)\n",
        "print('| The cumulative returns of LunarLander-v2 is ∈ (-1800, (-600, 200), 340)')"
      ],
      "metadata": {
        "id": "yH91VA17Hcsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4: Train DDPG on continuous action space task.**\n",
        "\n",
        "Train DDPG on [**Continuous action** space env `Pendulum`](https://gym.openai.com/envs/Pendulum-v0/)\n",
        "\n",
        "We show a cunstom env in helloworld/erl_env.py `class PendulumEnv`](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/helloworld/erl_env.py#L19-L23)\n",
        "\n",
        "OpenAI Pendulum env set its action space as (-2, +2). It is bad. We suggest that adjust action space to (-1, +1) when designing your own env.\n"
      ],
      "metadata": {
        "id": "z2Ik5cDoyPGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.config import Arguments\n",
        "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
        "from elegantrl_helloworld.env import get_gym_env_args\n",
        "from elegantrl_helloworld.agent import AgentDDPG\n",
        "agent_class = AgentDDPG\n",
        "\n",
        "from elegantrl_helloworld.env import PendulumEnv\n",
        "env = PendulumEnv('Pendulum-v0')  # PendulumEnv('Pendulum-v1')\n",
        "env_func = PendulumEnv\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Arguments(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.reward_scale = 2 ** -1  # RewardRange: -1800 < -200 < -50 < 0\n",
        "args.gamma = 0.97\n",
        "\n",
        "'''network update'''\n",
        "args.target_step = args.max_step * 2\n",
        "args.net_dim = 2 ** 7\n",
        "args.batch_size = 2 ** 7\n",
        "args.repeat_times = 2 ** 0\n",
        "args.explore_noise = 0.1\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 6\n",
        "args.eval_times = 2 ** 3\n",
        "args.break_step = int(1e5)\n",
        "\n",
        "args.learner_gpus = -1  # denotes use CPU\n",
        "train_agent(args)\n",
        "evaluate_agent(args)\n",
        "print('| The cumulative returns of Pendulum-v1 is ∈ (-1600, (-1400, -200), 0)')"
      ],
      "metadata": {
        "id": "wwkZXiHtyV6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n8zcgcn14uq"
      },
      "source": [
        "# **Part 5: Train PPO on continuous action space task.**\n",
        "\n",
        "Train PPO on [**Continuous action** space env `Pendulum`](https://gym.openai.com/envs/Pendulum-v0/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E03f6cTeajK4"
      },
      "source": [
        "from elegantrl_helloworld.config import Arguments\n",
        "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
        "from elegantrl_helloworld.env import get_gym_env_args\n",
        "from elegantrl_helloworld.agent import AgentPPO\n",
        "agent_class = AgentPPO\n",
        "\n",
        "from elegantrl_helloworld.env import PendulumEnv\n",
        "env = PendulumEnv()\n",
        "env_func = PendulumEnv\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Arguments(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.reward_scale = 2 ** -1  # RewardRange: -1800 < -200 < -50 < 0\n",
        "args.gamma = 0.97\n",
        "\n",
        "'''network update'''\n",
        "args.target_step = args.max_step * 8\n",
        "args.net_dim = 2 ** 7\n",
        "args.num_layer = 2\n",
        "args.batch_size = 2 ** 8\n",
        "args.repeat_times = 2 ** 5\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 6\n",
        "args.eval_times = 2 ** 3\n",
        "args.break_step = int(8e5)\n",
        "\n",
        "args.learner_gpus = -1\n",
        "train_agent(args)\n",
        "evaluate_agent(args)\n",
        "print('| The cumulative returns of Pendulum-v1 is ∈ (-1600, (-1400, -200), 0)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train PPO on [**Continuous action** space env `LunarLanderContinuous`](https://gym.openai.com/envs/LunarLanderContinuous-v2/)"
      ],
      "metadata": {
        "id": "rcFcUkwfzHLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.config import Arguments\n",
        "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
        "from elegantrl_helloworld.env import get_gym_env_args\n",
        "from elegantrl_helloworld.agent import AgentPPO\n",
        "agent_class = AgentPPO\n",
        "env_name = \"LunarLanderContinuous-v2\"\n",
        "\n",
        "import gym\n",
        "env = gym.make(env_name)\n",
        "env_func = gym.make\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Arguments(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.gamma = 0.99\n",
        "args.reward_scale = 2 ** -1\n",
        "\n",
        "'''network update'''\n",
        "args.target_step = args.max_step * 8\n",
        "args.num_layer = 3\n",
        "args.batch_size = 2 ** 7\n",
        "args.repeat_times = 2 ** 4\n",
        "args.lambda_entropy = 0.04\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 6\n",
        "args.eval_times = 2 ** 5\n",
        "args.break_step = int(4e5)\n",
        "\n",
        "args.learner_gpus = -1\n",
        "train_agent(args)\n",
        "evaluate_agent(args)\n",
        "print('| The cumulative returns of LunarLanderContinuous-v2 is ∈ (-1800, (-300, 200), 310+)')"
      ],
      "metadata": {
        "id": "9WCAcmIfzGyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1j5kLHF2dhJ"
      },
      "source": [
        "Train PPO on [**Continuous action** space env `BipedalWalker`](https://gym.openai.com/envs/BipedalWalker-v2/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGOPSD6da23k"
      },
      "source": [
        "from elegantrl_helloworld.config import Arguments\n",
        "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
        "from elegantrl_helloworld.env import get_gym_env_args\n",
        "from elegantrl_helloworld.agent import AgentPPO\n",
        "agent_class = AgentPPO\n",
        "env_name = \"BipedalWalker-v3\"\n",
        "\n",
        "import gym\n",
        "env = gym.make(env_name)\n",
        "env_func = gym.make\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Arguments(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.reward_scale = 2 ** -1\n",
        "args.gamma = 0.98\n",
        "\n",
        "'''network update'''\n",
        "args.target_step = args.max_step\n",
        "args.net_dim = 2 ** 8\n",
        "args.num_layer = 3\n",
        "args.batch_size = 2 ** 8\n",
        "args.repeat_times = 2 ** 4\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 6\n",
        "args.eval_times = 2 ** 4\n",
        "args.break_step = int(1e6)\n",
        "\n",
        "args.learner_gpus = -1\n",
        "train_agent(args)\n",
        "evaluate_agent(args)\n",
        "print('| The cumulative returns of BipedalWalker-v3 is ∈ (-150, (-100, 280), 320+)')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}