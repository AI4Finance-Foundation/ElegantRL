{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_helloworld_DQN_DDPG_PPO.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/ElegantRL/blob/master/tutorial_helloworld_DQN_DDPG_PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1gUG3OCJ5GS"
      },
      "source": [
        "# Demo: ElegantRL_HelloWorld_tutorial (DQN --> DDPG --> PPO)\n",
        "\n",
        "We suggest to following this order to quickly learn about RL:\n",
        "- DQN (Deep Q Network), a basic RL algorithms in discrete action space.\n",
        "- DDPG (Deep Deterministic Policy Gradient), a basic RL algorithm in continuous action space.\n",
        "- PPO (Proximal Policy Gradient), a widely used RL algorithms in continuous action space.\n",
        "\n",
        "If you have any suggestion about ElegantRL Helloworld, you can discuss them in [ElegantRL issues/135: Suggestions for elegant_helloworld](https://github.com/AI4Finance-Foundation/ElegantRL/issues/135), and we will keep an eye on this issue.\n",
        "ElegantRL's code, especially the Helloworld, really needs a lot of feedback to be better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbamGVHC3AeW"
      },
      "source": [
        "# **Part 1: Install ElegantRL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U35bhkUqOqbS",
        "outputId": "84a5a375-47d2-49bd-bf29-8b533d960856",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# install elegantrl library\n",
        "!pip install git+https://github.com/AI4Finance-LLC/ElegantRL.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/AI4Finance-LLC/ElegantRL.git\n",
            "  Cloning https://github.com/AI4Finance-LLC/ElegantRL.git to /tmp/pip-req-build-znnw9gvx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-LLC/ElegantRL.git /tmp/pip-req-build-znnw9gvx\n",
            "  Resolved https://github.com/AI4Finance-LLC/ElegantRL.git to commit 37aac1f592e1add9f9fd37ae8db1094656009b76\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: th in /usr/local/lib/python3.12/dist-packages (from ElegantRL==0.3.10) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ElegantRL==0.3.10) (2.0.2)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (from ElegantRL==0.3.10) (1.2.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from ElegantRL==0.3.10) (3.10.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->ElegantRL==0.3.10) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->ElegantRL==0.3.10) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium->ElegantRL==0.3.10) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->ElegantRL==0.3.10) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->ElegantRL==0.3.10) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->ElegantRL==0.3.10) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->ElegantRL==0.3.10) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->ElegantRL==0.3.10) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->ElegantRL==0.3.10) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->ElegantRL==0.3.10) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->ElegantRL==0.3.10) (2.9.0.post0)\n",
            "Requirement already satisfied: niltype<2.0,>=0.3 in /usr/local/lib/python3.12/dist-packages (from th->ElegantRL==0.3.10) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->ElegantRL==0.3.10) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Part 2: Import ElegantRL helloworld**\n",
        "\n",
        "We hope that the `ElegantRL Helloworld` would help people who want to learn about reinforcement learning to quickly run a few introductory examples.\n",
        "- **Less lines of code**. (code lines <1000)\n",
        "- **Less packages requirements**. (only `torch` and `gym` )\n",
        "- **keep a consistent style with the full version of ElegantRL**.\n",
        "\n",
        "![File_structure of ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL/raw/master/figs/File_structure.png)\n",
        "\n",
        "One sentence summary: an agent `agent.py` with Actor-Critic networks `net.py` is trained `run.py` by interacting with an environment `env.py`.\n",
        "\n",
        "\n",
        "In this tutorial, we only need to download the directory from [helloworld](https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/helloworld) using the following code.\n",
        "\n",
        "The files in `elegantrl_helloworld` including:\n",
        "`config.py`, `agent.py`, `net.py`, `env.py`, `run.py`"
      ],
      "metadata": {
        "id": "zJPivVxHMrAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r -f /content/ElegantRL/\n",
        "!rm -r -f /content/elegantrl_helloworld\n",
        "!git clone https://github.com/kuds/ElegantRL.git\n",
        "!mv /content/ElegantRL/helloworld /content/elegantrl_helloworld"
      ],
      "metadata": {
        "id": "sw_gE-IpovQ4",
        "outputId": "c3447481-dea4-4da2-aa0f-a9bcb86f195b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ElegantRL'...\n",
            "remote: Enumerating objects: 17325, done.\u001b[K\n",
            "remote: Counting objects: 100% (2326/2326), done.\u001b[K\n",
            "remote: Compressing objects: 100% (963/963), done.\u001b[K\n",
            "remote: Total 17325 (delta 1445), reused 1893 (delta 1285), pack-reused 14999 (from 4)\u001b[K\n",
            "Receiving objects: 100% (17325/17325), 115.60 MiB | 21.13 MiB/s, done.\n",
            "Resolving deltas: 100% (11187/11187), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "id": "DJrixoTnehAm",
        "outputId": "1f41ce59-c763-4c4f-d715-48269a857d95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Using cached swig-4.3.1.post0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Using cached swig-4.3.1.post0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "Installing collected packages: swig\n",
            "Successfully installed swig-4.3.1.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "id": "Mr4D_GGCeagS",
        "outputId": "4c1be9b1-f631-409b-b872-7031e73eed43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.3.1.post0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp312-cp312-linux_x86_64.whl size=2409504 sha256=1dc3fb949d238a3414c106726a1372a39187baf81a8c57d23de15f353d6ca8eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/e9/60/774da0bcd07f7dc7761a8590fa2d065e4069568e78dcdc3318\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.erl_run import train_agent, valid_agent\n",
        "from elegantrl_helloworld.erl_config import Config, get_gym_env_args"
      ],
      "metadata": {
        "id": "nweGpiR1M0yA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVdmpnK_3Zcn"
      },
      "source": [
        "## **Part 3: Train DQN on discreted action space task.**\n",
        "\n",
        "Train DQN on [**Discreted action** space task `CartPole`](https://gym.openai.com/envs/CartPole-v1/)\n",
        "\n",
        "You can see [/helloworld/erl_config.py](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/helloworld/erl_config.py) to get more information about hyperparameter.\n",
        "\n",
        "```\n",
        "class Arguments:\n",
        "    def __init__(self, agent_class, env_func=None, env_args=None):\n",
        "        self.env_num = self.env_args['env_num']  # env_num = 1. In vector env, env_num > 1.\n",
        "        self.max_step = self.env_args['max_step']  # the max step of an episode\n",
        "        self.env_name = self.env_args['env_name']  # the env name. Be used to set 'cwd'.\n",
        "        self.state_dim = self.env_args['state_dim']  # vector dimension (feature number) of state\n",
        "        self.action_dim = self.env_args['action_dim']  # vector dimension (feature number) of action\n",
        "        self.if_discrete = self.env_args['if_discrete']  # discrete or continuous action space\n",
        "        ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.erl_config import Config\n",
        "from elegantrl_helloworld.erl_agent import AgentDQN\n",
        "agent_class = AgentDQN\n",
        "env_name = \"CartPole-v1\"\n",
        "\n",
        "import gymnasium as gym\n",
        "env = gym.make(env_name)\n",
        "env_func = gym.make\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Config(agent_class, env_func, env_args)\n",
        "\n",
        "# Set attributes from env_args on the args object\n",
        "args.state_dim = env_args['state_dim']\n",
        "args.action_dim = env_args['action_dim']\n",
        "args.if_discrete = env_args['if_discrete']\n",
        "# args.env_num = env_args['env_num'] # Assuming env_num is in env_args based on LunarLander example output\n",
        "\n",
        "\n",
        "'''reward shaping'''\n",
        "args.reward_scale = 2 ** 0  # an approximate target reward usually be closed to 256\n",
        "args.gamma = 0.97  # discount factor of future rewards\n",
        "\n",
        "'''network update'''\n",
        "args.net_dim = 2 ** 7  # the middle layer dimension of Fully Connected Network\n",
        "args.num_layer = 3  # the layer number of MultiLayer Perceptron, `assert num_layer >= 2`\n",
        "args.batch_size = 2 ** 7  # num of transitions sampled from replay buffer.\n",
        "args.repeat_times = 2 ** 0  # repeatedly update network using ReplayBuffer to keep critic's loss small\n",
        "args.explore_rate = 0.25  # epsilon-greedy for exploration.\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 5  # number of times that get episode return\n",
        "args.eval_times = 2 ** 3  # number of times that get episode return\n",
        "args.break_step = int(8e4)  # break training if 'total_step > break_step'"
      ],
      "metadata": {
        "id": "AAPdjovQrTpE",
        "outputId": "15fd62a0-c611-4025-9c15-39b6a53d4539",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env_args = {'env_name': 'CartPole-v1',\n",
            "            'state_dim': 4,\n",
            "            'action_dim': np.int64(2),\n",
            "            'if_discrete': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose gpu id `0` using `args.learner_gpu = 0`. Set as `-1` or GPU is unavaliable, the training program will choose CPU automatically.\n",
        "\n",
        "- The cumulative returns of CartPole-v0  is ∈ (0, (1, 195), 200)\n",
        "- The cumulative returns of task_name is ∈ (min score, (score of random action, target score), max score)."
      ],
      "metadata": {
        "id": "Rq5LPOH2B0aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# args.learner_gpus = -1\n",
        "import os\n",
        "env_class = gym.make\n",
        "train_agent(args)\n",
        "actor_name = sorted([s for s in os.listdir(args.cwd) if s[-4:] == '.pth'])[-1]\n",
        "actor_path = f\"{args.cwd}/{actor_name}\"\n",
        "valid_agent(env_class, env_args, args.net_dims, agent_class, actor_path)\n",
        "print(f'| The cumulative returns of {env_name}  is ∈ (0, (1, 195), 200)')"
      ],
      "metadata": {
        "id": "n7SBwVAkA8lA",
        "outputId": "4427da02-5cde-4fd9-892e-8d83c97fe6f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Arguments Remove cwd: ./CartPole-v1_DQN_0\n",
            "| Evaluator:\n",
            "| `step`: Number of samples, or total training steps, or running times of `env.step()`.\n",
            "| `time`: Time spent from the start of training to this moment.\n",
            "| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\n",
            "| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\n",
            "| `avgS`: Average of steps in an episode.\n",
            "| `objC`: Objective of Critic network. Or call it loss function of critic network.\n",
            "| `objA`: Objective of Actor network. It is the average Q value of the critic network.\n",
            "|     step      time  |     avgR    stdR    avgS  |     objC      objA\n",
            "| 1.02e+04        12  |     9.25    0.83       9  |     0.14      2.85\n",
            "| 2.05e+04        23  |    10.00    0.87      10  |     1.17     18.66\n",
            "| 3.07e+04        39  |     9.25    0.97       9  |     1.39     26.08\n",
            "| 4.10e+04        58  |     9.75    0.66      10  |     3.76     60.30\n",
            "| 5.12e+04        82  |    61.62    4.58      62  |     3.82     60.02\n",
            "| 6.14e+04       109  |   145.62   20.36     146  |     2.03     43.65\n",
            "| 7.17e+04       140  |   153.25   26.14     153  |     1.25     37.51\n",
            "| Save learning curve in ./CartPole-v1_DQN_0/LearningCurve.jpg\n",
            "| render and load actor from: ./CartPole-v1_DQN_0/actor_000000071680_00000140_00153.25.pth\n",
            "|   0  cumulative_reward   262.000  episode_step   262\n",
            "|   1  cumulative_reward   139.000  episode_step   139\n",
            "|   2  cumulative_reward   217.000  episode_step   217\n",
            "|   3  cumulative_reward   101.000  episode_step   101\n",
            "|   4  cumulative_reward   207.000  episode_step   207\n",
            "|   5  cumulative_reward   130.000  episode_step   130\n",
            "|   6  cumulative_reward   109.000  episode_step   109\n",
            "|   7  cumulative_reward   141.000  episode_step   141\n",
            "| The cumulative returns of CartPole-v1  is ∈ (0, (1, 195), 200)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/envs/classic_control/cartpole.py:250: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train DQN on [**Discreted action** space env `LunarLander`](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
        "\n",
        "**You can pass and run codes below.**. Because DQN takes over 6000 seconds for training. It is too slow. (DuelingDoubleDQN taks less than 1000 second for training on LunarLander-v3 task.)\n",
        "\n",
        "And there are many other DQN variance algorithms which get higher cumulative returns and takes less time for training. See [examples/demo_DQN_Dueling_Double_DQN.py](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/examples/demo_DQN_Dueling_Double_DQN.py)"
      ],
      "metadata": {
        "id": "qK21xTxnHGOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.erl_agent import AgentDQN\n",
        "agent_class = AgentDQN\n",
        "env_name = \"LunarLander-v3\"\n",
        "\n",
        "import gymnasium as gym\n",
        "env = gym.make(env_name)\n",
        "env_func = gym.make\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Config(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.reward_scale = 2 ** 0\n",
        "args.gamma = 0.99\n",
        "\n",
        "'''network update'''\n",
        "args.net_dim = 2 ** 7\n",
        "args.num_layer = 3\n",
        "args.batch_size = 2 ** 6\n",
        "args.repeat_times = 2 ** 0\n",
        "args.explore_noise = 0.125\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 7\n",
        "args.eval_times = 2 ** 4\n",
        "args.break_step = int(4e5)  # LunarLander needs a larger `break_step`\n",
        "\n",
        "# args.learner_gpus = -1  # denotes use CPU\n",
        "train_agent(args)\n",
        "actor_name = sorted([s for s in os.listdir(args.cwd) if s[-4:] == '.pth'])[-1]\n",
        "actor_path = f\"{args.cwd}/{actor_name}\"\n",
        "valid_agent(env_func, env_args, args.net_dims, agent_class, actor_path)\n",
        "print(f'| The cumulative returns of {env_name} is ∈ (-1800, (-600, 200), 340)')"
      ],
      "metadata": {
        "id": "yH91VA17Hcsn",
        "outputId": "218af887-155b-4bec-9b0e-0d6c056dbd51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env_args = {'env_name': 'LunarLander-v3',\n",
            "            'state_dim': 8,\n",
            "            'action_dim': np.int64(4),\n",
            "            'if_discrete': True}\n",
            "| Arguments Remove cwd: ./LunarLander-v3_DQN_0\n",
            "| Evaluator:\n",
            "| `step`: Number of samples, or total training steps, or running times of `env.step()`.\n",
            "| `time`: Time spent from the start of training to this moment.\n",
            "| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\n",
            "| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\n",
            "| `avgS`: Average of steps in an episode.\n",
            "| `objC`: Objective of Critic network. Or call it loss function of critic network.\n",
            "| `objA`: Objective of Actor network. It is the average Q value of the critic network.\n",
            "|     step      time  |     avgR    stdR    avgS  |     objC      objA\n",
            "| 1.02e+04        20  |  -188.37  114.99     151  |     1.38    -34.81\n",
            "| 2.05e+04        45  |  -286.70   20.31     121  |     2.37    -64.29\n",
            "| 3.07e+04        80  |  -192.04   28.14     399  |     2.78    -67.73\n",
            "| 4.10e+04       124  |   -92.85   42.88    1000  |     2.66    -41.83\n",
            "| 5.12e+04       177  |   -98.56   39.68     988  |     2.18      2.98\n",
            "| 6.14e+04       239  |  -102.83   26.61    1000  |     2.16     19.80\n",
            "| 7.17e+04       304  |  -202.41   35.46     477  |     1.89     20.27\n",
            "| 8.19e+04       374  |  -173.57   96.79     221  |     1.67     14.56\n",
            "| 9.22e+04       455  |   -85.74  151.54     566  |     1.74      4.89\n",
            "| 1.02e+05       546  |  -163.22  118.54     766  |     1.66     11.57\n",
            "| 1.13e+05       645  |  -182.68  161.93     956  |     1.66      6.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4: Train DDPG on continuous action space task.**\n",
        "\n",
        "Train DDPG on [**Continuous action** space env `Pendulum`](https://gymnasium.farama.org/environments/classic_control/pendulum/)\n",
        "\n",
        "We show a cunstom env in helloworld/erl_env.py `class PendulumEnv`](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/helloworld/erl_env.py#L19-L23)\n",
        "\n",
        "OpenAI Pendulum env set its action space as (-2, +2). It is bad. We suggest that adjust action space to (-1, +1) when designing your own env.\n"
      ],
      "metadata": {
        "id": "z2Ik5cDoyPGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.erl_config import Config, get_gym_env_args\n",
        "from elegantrl_helloworld.erl_run import train_agent, valid_agent\n",
        "from elegantrl_helloworld.erl_env import PendulumEnv\n",
        "from elegantrl_helloworld.erl_agent import AgentDDPG\n",
        "agent_class = AgentDDPG\n",
        "\n",
        "env_name = \"Pendulum-v1\"\n",
        "env = PendulumEnv(env_name)  # PendulumEnv('Pendulum-v1')\n",
        "env_func = PendulumEnv\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Arguments(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.reward_scale = 2 ** -1  # RewardRange: -1800 < -200 < -50 < 0\n",
        "args.gamma = 0.97\n",
        "\n",
        "'''network update'''\n",
        "args.target_step = args.max_step * 2\n",
        "args.net_dim = 2 ** 7\n",
        "args.batch_size = 2 ** 7\n",
        "args.repeat_times = 2 ** 0\n",
        "args.explore_noise = 0.1\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 6\n",
        "args.eval_times = 2 ** 3\n",
        "args.break_step = int(1e5)\n",
        "\n",
        "args.learner_gpus = -1  # denotes use CPU\n",
        "train_agent(args)\n",
        "evaluate_agent(args)\n",
        "print(f'| The cumulative returns of {env_name} is ∈ (-1600, (-1400, -200), 0)')"
      ],
      "metadata": {
        "id": "wwkZXiHtyV6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n8zcgcn14uq"
      },
      "source": [
        "# **Part 5: Train PPO on continuous action space task.**\n",
        "\n",
        "Train PPO on [**Continuous action** space env `Pendulum`](https://gym.openai.com/envs/Pendulum-v0/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E03f6cTeajK4"
      },
      "source": [
        "from elegantrl_helloworld.config import Arguments\n",
        "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
        "from elegantrl_helloworld.env import get_gym_env_args\n",
        "from elegantrl_helloworld.agent import AgentPPO\n",
        "agent_class = AgentPPO\n",
        "\n",
        "from elegantrl_helloworld.env import PendulumEnv\n",
        "env = PendulumEnv()\n",
        "env_func = PendulumEnv\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Arguments(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.reward_scale = 2 ** -1  # RewardRange: -1800 < -200 < -50 < 0\n",
        "args.gamma = 0.97\n",
        "\n",
        "'''network update'''\n",
        "args.target_step = args.max_step * 8\n",
        "args.net_dim = 2 ** 7\n",
        "args.num_layer = 2\n",
        "args.batch_size = 2 ** 8\n",
        "args.repeat_times = 2 ** 5\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 6\n",
        "args.eval_times = 2 ** 3\n",
        "args.break_step = int(8e5)\n",
        "\n",
        "args.learner_gpus = -1\n",
        "train_agent(args)\n",
        "evaluate_agent(args)\n",
        "print('| The cumulative returns of Pendulum-v1 is ∈ (-1600, (-1400, -200), 0)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train PPO on [**Continuous action** space env `LunarLanderContinuous`](https://gym.openai.com/envs/LunarLanderContinuous-v2/)"
      ],
      "metadata": {
        "id": "rcFcUkwfzHLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.config import Arguments\n",
        "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
        "from elegantrl_helloworld.env import get_gym_env_args\n",
        "from elegantrl_helloworld.agent import AgentPPO\n",
        "agent_class = AgentPPO\n",
        "env_name = \"LunarLanderContinuous-v3\"\n",
        "\n",
        "import gymnasium as gym\n",
        "env = gym.make(env_name)\n",
        "env_func = gym.make\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Arguments(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.gamma = 0.99\n",
        "args.reward_scale = 2 ** -1\n",
        "\n",
        "'''network update'''\n",
        "args.target_step = args.max_step * 8\n",
        "args.num_layer = 3\n",
        "args.batch_size = 2 ** 7\n",
        "args.repeat_times = 2 ** 4\n",
        "args.lambda_entropy = 0.04\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 6\n",
        "args.eval_times = 2 ** 5\n",
        "args.break_step = int(4e5)\n",
        "\n",
        "args.learner_gpus = -1\n",
        "train_agent(args)\n",
        "evaluate_agent(args)\n",
        "print(f'| The cumulative returns of {env_name} is ∈ (-1800, (-300, 200), 310+)')"
      ],
      "metadata": {
        "id": "9WCAcmIfzGyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1j5kLHF2dhJ"
      },
      "source": [
        "Train PPO on [**Continuous action** space env `BipedalWalker`](https://gym.openai.com/envs/BipedalWalker-v2/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGOPSD6da23k"
      },
      "source": [
        "from elegantrl_helloworld.config import Arguments\n",
        "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
        "from elegantrl_helloworld.env import get_gym_env_args\n",
        "from elegantrl_helloworld.agent import AgentPPO\n",
        "agent_class = AgentPPO\n",
        "env_name = \"BipedalWalker-v3\"\n",
        "\n",
        "import gymnasium as gym\n",
        "env = gym.make(env_name)\n",
        "env_func = gym.make\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Arguments(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.reward_scale = 2 ** -1\n",
        "args.gamma = 0.98\n",
        "\n",
        "'''network update'''\n",
        "args.target_step = args.max_step\n",
        "args.net_dim = 2 ** 8\n",
        "args.num_layer = 3\n",
        "args.batch_size = 2 ** 8\n",
        "args.repeat_times = 2 ** 4\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 6\n",
        "args.eval_times = 2 ** 4\n",
        "args.break_step = int(1e6)\n",
        "\n",
        "args.learner_gpus = -1\n",
        "train_agent(args)\n",
        "evaluate_agent(args)\n",
        "print(f'| The cumulative returns of {env_name} is ∈ (-150, (-100, 280), 320+)')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}